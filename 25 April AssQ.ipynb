{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ca6294-c2f0-472f-a332-e56d1aa3fd52",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae401b-f8de-41c1-b904-6f65859cb3b1",
   "metadata": {},
   "source": [
    "Assignment Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d7d732-aa21-49cd-b078-731a7ce4fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5605848-ecbc-4fb6-94a8-dd291cb5ac3f",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors are essential concepts in linear algebra, particularly in the context of matrix operations. They play a crucial role in various mathematical applications, including eigen-decomposition, which involves breaking down a matrix into its eigenvalues and corresponding eigenvectors.\n",
    "\n",
    "**Eigenvalues:** Eigenvalues are scalars associated with a square matrix. For a matrix A, an eigenvalue (λ) is a scalar that, when multiplied by its corresponding eigenvector, results in the original vector scaled by that eigenvalue. Mathematically, if v is an eigenvector of A corresponding to the eigenvalue λ, it satisfies the equation:\n",
    "\n",
    "A * v = λ * v\n",
    "\n",
    "**Eigenvectors:** Eigenvectors are non-zero vectors that, when transformed by a square matrix, retain their direction (up to scaling) but might change in magnitude. They represent directions in the space that are only scaled by the linear transformation represented by the matrix.\n",
    "\n",
    "**Eigen-Decomposition:** Eigen-decomposition is a method used for diagonalizing a matrix A by expressing it as a product of eigenvectors and a diagonal matrix of eigenvalues. For a matrix A (assuming it's diagonalizable), the eigen-decomposition is given by:\n",
    "\n",
    "A = P * D * P^(-1)\n",
    "\n",
    "Where:\n",
    "- P is a matrix whose columns are the eigenvectors of A.\n",
    "- D is a diagonal matrix containing the corresponding eigenvalues along the diagonal.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider a 2x2 matrix A:\n",
    "\n",
    "\\[ A = \\begin{bmatrix} 4 & 2 \\\\ 1 & 3 \\end{bmatrix} \\]\n",
    "\n",
    "To find the eigenvalues (λ) and eigenvectors (v), we solve the equation:\n",
    "\n",
    "\\[ A * v = λ * v \\]\n",
    "\n",
    "For eigenvalues, we compute the characteristic equation:\n",
    "\n",
    "\\[ |A - λI| = 0 \\]\n",
    "\n",
    "Where I is the identity matrix.\n",
    "\n",
    "The characteristic equation for matrix A:\n",
    "\n",
    "\\[ |A - λI| = \\begin{vmatrix} 4-λ & 2 \\\\ 1 & 3-λ \\end{vmatrix} = (4-λ)(3-λ) - 2*1 = λ^2 - 7λ + 10 = 0 \\]\n",
    "\n",
    "Solving this quadratic equation gives us eigenvalues:\n",
    "\n",
    "\\[ λ_1 = 5 \\]\n",
    "\\[ λ_2 = 2 \\]\n",
    "\n",
    "Now, for each eigenvalue, we find its corresponding eigenvector by solving:\n",
    "\n",
    "For λ = 5:\n",
    "\\[ (A - 5I) * v_1 = 0 \\]\n",
    "\\[ \\begin{bmatrix} -1 & 2 \\\\ 1 & -2 \\end{bmatrix} * \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\]\n",
    "Solving this system of equations gives the eigenvector corresponding to λ = 5: \\( v_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\)\n",
    "\n",
    "For λ = 2:\n",
    "\\[ (A - 2I) * v_2 = 0 \\]\n",
    "\\[ \\begin{bmatrix} 2 & 2 \\\\ 1 & 1 \\end{bmatrix} * \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\]\n",
    "Solving this system of equations gives the eigenvector corresponding to λ = 2: \\( v_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} \\)\n",
    "\n",
    "Therefore, the eigenvalues of matrix A are 5 and 2, and their corresponding eigenvectors are \\( v_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\) and \\( v_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} \\), respectively.\n",
    "\n",
    "Eigen-decomposition involves forming a matrix P using these eigenvectors and a diagonal matrix D with eigenvalues:\n",
    "\n",
    "\\[ P = \\begin{bmatrix} 2 & -1 \\\\ 1 & 1 \\end{bmatrix} \\]\n",
    "\\[ D = \\begin{bmatrix} 5 & 0 \\\\ 0 & 2 \\end{bmatrix} \\]\n",
    "\n",
    "And we verify if \\( A = P * D * P^{-1} \\) to complete the eigen-decomposition process.\n",
    "\n",
    "This example illustrates how eigenvalues and eigenvectors are computed for a matrix and how eigen-decomposition breaks down the matrix using these eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abee09f-8ee7-4e4e-9db9-3931c82d092e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7798a9-4042-45fe-887b-063fe6a291c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What is eigen decomposition and what is its significance in linear algebra?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d624f91-5a8d-4682-9d42-c409b9fa978a",
   "metadata": {},
   "source": [
    "Eigen decomposition is a fundamental concept in linear algebra that involves breaking down a square matrix into its constituent parts of eigenvalues and eigenvectors. For a given square matrix \\(A\\), the eigen decomposition expresses \\(A\\) as a product of matrices containing its eigenvectors and eigenvalues.\n",
    "\n",
    "The eigen decomposition of a matrix \\(A\\) can be represented as:\n",
    "\n",
    "\\[A = P \\cdot \\Lambda \\cdot P^{-1}\\]\n",
    "\n",
    "Where:\n",
    "- \\(P\\) is a matrix whose columns are the eigenvectors of \\(A\\).\n",
    "- \\(\\Lambda\\) is a diagonal matrix containing the eigenvalues of \\(A\\) on its diagonal.\n",
    "- \\(P^{-1}\\) is the inverse of matrix \\(P\\).\n",
    "\n",
    "This decomposition is applicable to diagonalizable matrices, which are square matrices that have a full set of linearly independent eigenvectors.\n",
    "\n",
    "Significance of Eigen Decomposition in Linear Algebra:\n",
    "\n",
    "1. **Spectral Analysis:** Eigen decomposition is fundamental in understanding the spectral properties of a matrix. It provides insights into the behavior of linear transformations represented by matrices. Eigenvalues represent the scaling factors applied to the corresponding eigenvectors, showing how the matrix affects those directions in space.\n",
    "\n",
    "2. **Diagonalization:** Eigen decomposition allows for the diagonalization of a matrix, which simplifies matrix operations. Diagonal matrices are easier to work with in various computations compared to non-diagonal matrices.\n",
    "\n",
    "3. **Factorization and Solving Systems of Equations:** Eigen decomposition can aid in solving systems of linear equations. It simplifies matrix exponentiation, matrix powers, and other matrix operations, making computations more efficient.\n",
    "\n",
    "4. **Principal Component Analysis (PCA):** Eigen decomposition is used in PCA for reducing the dimensionality of data. PCA finds the eigenvectors and eigenvalues of the covariance matrix, enabling the transformation of data into a new space of reduced dimensions.\n",
    "\n",
    "5. **Physical Systems and Sciences:** In physics and various scientific fields, eigen decomposition finds application in analyzing dynamical systems, quantum mechanics, vibration analysis, and more. Eigenvalues and eigenvectors help in understanding the behavior of physical systems under certain transformations or operations.\n",
    "\n",
    "6. **Numerical Methods and Algorithms:** Eigen decomposition serves as a basis for various numerical methods and algorithms in machine learning, signal processing, and optimization problems.\n",
    "\n",
    "Overall, eigen decomposition is a powerful tool in linear algebra with broad applications in various fields, providing a deeper understanding of matrix properties and facilitating efficient computations and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25736cbd-9218-4f83-9ffd-769e47349da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ba90fcea-cf2d-4711-ab5c-b28497d70f32",
   "metadata": {},
   "source": [
    "Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the\n",
    "Eigen-Decomposition approach? Provide a brief proof to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea158c-373b-4b69-a3fd-bdf994c0f8e2",
   "metadata": {},
   "source": [
    "For a square matrix to be diagonalizable using the Eigen-Decomposition approach, several conditions must be met:\n",
    "\n",
    "1. **Matrix Size:** The matrix must be square, meaning it should have an equal number of rows and columns.\n",
    "\n",
    "2. **Linearly Independent Eigenvectors:** The matrix should have a full set of linearly independent eigenvectors corresponding to its eigenvalues.\n",
    "\n",
    "3. **Geometric Multiplicity Equals Algebraic Multiplicity:** Each eigenvalue's geometric multiplicity (the number of linearly independent eigenvectors corresponding to an eigenvalue) should equal its algebraic multiplicity (the number of times the eigenvalue appears as a root of the characteristic polynomial).\n",
    "\n",
    "Proof:\n",
    "\n",
    "Let's consider a square matrix \\(A\\) of size \\(n \\times n\\) that possesses \\(n\\) distinct eigenvalues \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\).\n",
    "\n",
    "For each eigenvalue \\(\\lambda_i\\), let \\(m_i\\) be its algebraic multiplicity, which is the number of times \\(\\lambda_i\\) appears as a root of the characteristic polynomial of \\(A\\).\n",
    "\n",
    "The geometric multiplicity of an eigenvalue \\(\\lambda_i\\) (denoted by \\(g_i\\)) is the number of linearly independent eigenvectors corresponding to that eigenvalue. The total number of linearly independent eigenvectors for matrix \\(A\\) must be \\(n\\), which is the size of the matrix.\n",
    "\n",
    "Now, the sum of the geometric multiplicities (\\(g_i\\)) of all eigenvalues must be equal to the matrix's size \\(n\\) (the total number of linearly independent eigenvectors):\n",
    "\n",
    "\\[g_1 + g_2 + \\ldots + g_n = n\\]\n",
    "\n",
    "Also, the sum of the algebraic multiplicities (\\(m_i\\)) of all eigenvalues must also be equal to the matrix's size \\(n\\) (since it's an \\(n \\times n\\) matrix):\n",
    "\n",
    "\\[m_1 + m_2 + \\ldots + m_n = n\\]\n",
    "\n",
    "Therefore, for a matrix to be diagonalizable using Eigen-Decomposition, it's necessary that the geometric multiplicity of each eigenvalue equals its algebraic multiplicity:\n",
    "\n",
    "\\[g_i = m_i\\]\n",
    "\n",
    "This condition ensures that the matrix has a complete set of \\(n\\) linearly independent eigenvectors (corresponding to \\(n\\) distinct eigenvalues), allowing the matrix to be diagonalized via Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3160eb-1045-4b08-9cd2-20bdce6dce00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ba681fcb-6e4a-414a-967a-3ad180132d5e",
   "metadata": {},
   "source": [
    "Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach?\n",
    "How is it related to the diagonalizability of a matrix? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4195aae2-0479-4fa2-88f4-550284ca4dc8",
   "metadata": {},
   "source": [
    "The Spectral Theorem is a crucial concept in linear algebra that establishes a profound connection between the diagonalizability of a matrix and its eigenvectors/eigenvalues. It provides conditions under which a matrix can be diagonalized, particularly when dealing with symmetric matrices.\n",
    "\n",
    "**Significance of the Spectral Theorem in the Context of Eigen-Decomposition:**\n",
    "\n",
    "The Spectral Theorem states that for a symmetric matrix, not only are there \\(n\\) linearly independent eigenvectors (where \\(n\\) is the size of the matrix), but these eigenvectors are orthogonal to each other. Additionally, the corresponding eigenvalues are real numbers.\n",
    "\n",
    "In the context of the Eigen-Decomposition approach, the Spectral Theorem guarantees that for a symmetric matrix \\(A\\):\n",
    "\n",
    "1. It has a full set of \\(n\\) linearly independent eigenvectors (\\(n\\) being the matrix size).\n",
    "2. These eigenvectors form an orthogonal basis for the vector space.\n",
    "3. The matrix \\(A\\) is diagonalizable by expressing it as \\(A = P \\cdot \\Lambda \\cdot P^{-1}\\), where \\(P\\) is an orthogonal matrix containing the eigenvectors, and \\(\\Lambda\\) is a diagonal matrix of eigenvalues.\n",
    "\n",
    "**Relation to Diagonalizability:**\n",
    "\n",
    "The Spectral Theorem is directly related to the diagonalizability of symmetric matrices. It ensures that symmetric matrices possess a complete set of orthogonal eigenvectors, allowing them to be diagonalized via the Eigen-Decomposition approach.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider a symmetric matrix:\n",
    "\n",
    "\\[ A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} \\]\n",
    "\n",
    "This matrix is symmetric, and its eigenvalues and eigenvectors can be calculated. The eigenvalues (\\(\\lambda\\)) and corresponding eigenvectors (\\(v\\)) are:\n",
    "\n",
    "Eigenvalues:\n",
    "\\[ \\lambda_1 = 4, \\quad v_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\]\n",
    "\\[ \\lambda_2 = 2, \\quad v_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\]\n",
    "\n",
    "These eigenvalues are real, and the eigenvectors are orthogonal (\\(v_1\\) and \\(v_2\\) are orthogonal).\n",
    "\n",
    "Using the Spectral Theorem, since matrix \\(A\\) is symmetric, it can be diagonalized as:\n",
    "\n",
    "\\[ A = P \\cdot \\Lambda \\cdot P^{-1} \\]\n",
    "\n",
    "Where \\(P\\) is a matrix whose columns are the orthogonal eigenvectors, and \\(\\Lambda\\) is a diagonal matrix containing the eigenvalues.\n",
    "\n",
    "In this case:\n",
    "\\[ P = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\]\n",
    "\\[ \\Lambda = \\begin{bmatrix} 4 & 0 \\\\ 0 & 2 \\end{bmatrix} \\]\n",
    "\n",
    "Therefore, using the Spectral Theorem, we've shown how a symmetric matrix \\(A\\) can be diagonalized via Eigen-Decomposition due to its possession of orthogonal eigenvectors and real eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428cd5c3-a886-46ca-91f8-688b5e116913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ac1b2-347a-4799-9295-a549a7bafe3d",
   "metadata": {},
   "source": [
    "The eigenvalues of a matrix can be found by solving the characteristic equation associated with that matrix. Eigenvalues play a fundamental role in linear algebra and matrix operations, providing critical information about the transformation represented by the matrix.\n",
    "\n",
    "Given a square matrix \\(A\\), its eigenvalues (\\(\\lambda\\)) are solutions to the characteristic equation:\n",
    "\n",
    "\\[ \\text{det}(A - \\lambda I) = 0 \\]\n",
    "\n",
    "Where:\n",
    "- \\(A\\) is the square matrix.\n",
    "- \\(\\lambda\\) represents the eigenvalue we are trying to find.\n",
    "- \\(I\\) is the identity matrix of the same size as \\(A\\).\n",
    "- \\(\\text{det}(\\cdot)\\) denotes the determinant.\n",
    "\n",
    "The characteristic equation is derived by subtracting \\(\\lambda\\) times the identity matrix from \\(A\\) and then taking the determinant of the resulting matrix, setting it equal to zero.\n",
    "\n",
    "Once the characteristic equation is formed, solving it for \\(\\lambda\\) yields the eigenvalues of matrix \\(A\\). These eigenvalues are the roots of the characteristic polynomial, and they might be real or complex numbers, depending on the matrix.\n",
    "\n",
    "**What Eigenvalues Represent:**\n",
    "\n",
    "Eigenvalues hold essential information about the linear transformation represented by the matrix \\(A\\). Each eigenvalue is associated with its corresponding eigenvector and represents a scaling factor by which the eigenvector is stretched or shrunk when the linear transformation is applied.\n",
    "\n",
    "Specifically:\n",
    "- **Eigenvalues determine the scale:** If an eigenvector is transformed by the matrix \\(A\\), the resulting transformed vector is a scalar multiple of the original eigenvector, with the eigenvalue being the scalar factor.\n",
    "  \n",
    "- **Eigenvalues indicate transformation behavior:** The eigenvalues provide insights into how the linear transformation affects different directions or dimensions in space. For example, positive eigenvalues indicate stretching along those dimensions, negative eigenvalues indicate flipping or reflection, and zero eigenvalues represent vectors that collapse into the origin or are part of the null space.\n",
    "\n",
    "- **Matrix properties:** Eigenvalues are crucial in determining other matrix properties, such as determinant, trace, rank, and invertibility, as these properties are related to the eigenvalues.\n",
    "\n",
    "In summary, eigenvalues of a matrix represent the scaling factors by which the corresponding eigenvectors are stretched or shrunk under the linear transformation represented by the matrix. They provide valuable information about the behavior and properties of the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d922cc-5f46-49ac-8089-e75878a0d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. What are eigenvectors and how are they related to eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca8d6b-62a7-40c9-8512-b1472cd87061",
   "metadata": {},
   "source": [
    "Eigenvectors are special vectors associated with square matrices that have distinct properties when multiplied by the matrix they correspond to. They are directly linked to eigenvalues and play a crucial role in understanding the behavior of linear transformations represented by matrices.\n",
    "\n",
    "**Eigenvectors:**\n",
    "\n",
    "For a square matrix \\(A\\), an eigenvector \\(v\\) is a non-zero vector that, when multiplied by \\(A\\), results in a new vector that is a scalar multiple of the original eigenvector \\(v\\):\n",
    "\n",
    "\\[ A \\cdot v = \\lambda \\cdot v \\]\n",
    "\n",
    "Where:\n",
    "- \\(A\\) is the square matrix.\n",
    "- \\(v\\) is the eigenvector.\n",
    "- \\(\\lambda\\) is the corresponding eigenvalue.\n",
    "\n",
    "In essence, when matrix \\(A\\) operates on the eigenvector \\(v\\), the resulting vector is parallel to the original \\(v\\) but scaled by a scalar factor represented by the eigenvalue \\(\\lambda\\).\n",
    "\n",
    "**Relation to Eigenvalues:**\n",
    "\n",
    "Eigenvectors and eigenvalues are intimately related. Each eigenvector \\(v\\) of matrix \\(A\\) is associated with a corresponding eigenvalue \\(\\lambda\\). The equation \\(A \\cdot v = \\lambda \\cdot v\\) signifies this relationship.\n",
    "\n",
    "- **Eigenvalues determine the scaling factor:** Eigenvalues (\\(\\lambda\\)) determine how much the corresponding eigenvectors (\\(v\\)) are scaled or stretched when matrix \\(A\\) operates on them. The eigenvectors \\(v\\) retain their direction but may change in magnitude by a factor of the eigenvalue.\n",
    "\n",
    "- **Eigenvalues identify transformation behavior:** Eigenvectors represent directions or axes in space that remain unchanged in direction but only scale during the linear transformation represented by matrix \\(A\\). Eigenvalues specify how much the transformation scales these eigenvectors along their respective directions.\n",
    "\n",
    "- **Linearly independent eigenvectors:** A matrix can have multiple eigenvectors, each corresponding to a different eigenvalue. The set of linearly independent eigenvectors forms a basis that can be used to diagonalize the matrix, as in the eigen-decomposition.\n",
    "\n",
    "In summary, eigenvectors are vectors that, when transformed by a matrix, only change in magnitude, with their direction remaining unchanged, and are associated with specific eigenvalues that determine the scaling factor of these vectors under the transformation represented by the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f5efc-0e09-45dd-9a12-e1ad691b6167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
